{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce487305-42fe-4dd2-97d8-618d3f41fba7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import numpy as np\n",
    "from statsmodels.tsa.arima.model import ARIMA as ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39e6eec",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f96a269-9a38-41cc-acef-cb784b97aabd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import Data, Train & Holiday_Events\n",
    "store = pd.read_csv(\"store-sales-time-series-forecasting/train.csv\")\n",
    "holiday_events = pd.read_excel(\"store-sales-time-series-forecasting/holidays_events.xlsx\")\n",
    "\n",
    "print(store.head())\n",
    "print(\"\\n\")\n",
    "print(holiday_events.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e280b5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Data, Keep 'date' & 'type'\n",
    "holiday_events = holiday_events[['date', 'type']]\n",
    "\n",
    "# Change all 'type' Object to Float 1\n",
    "holiday_events['type'] = 1\n",
    "\n",
    "# Change 'type' to 'holiday'\n",
    "holiday_events = holiday_events.rename(columns={'type': 'holiday'})\n",
    "\n",
    "# Change Object to DateTime64\n",
    "store['date'] = pd.to_datetime(store['date'])\n",
    "\n",
    "# Compile Data\n",
    "csv = pd.merge(store, holiday_events, on='date', how='outer')\n",
    "csv = csv.fillna(0)\n",
    "\n",
    "# Clean Data, Keep any food related 'family'\n",
    "csv = csv[(csv['family'] == 'BREAD/BAKERY') |\n",
    "          (csv['family'] == 'DAIRY') |\n",
    "          (csv['family'] == 'DELI') |\n",
    "          (csv['family'] == 'EGGS') |\n",
    "          (csv['family'] == 'FROZEN FOODS') |\n",
    "          (csv['family'] == 'MEATS') | \n",
    "          (csv['family'] == 'POULTRY') |\n",
    "          (csv['family'] == 'PREPARED FOODS') | \n",
    "          (csv['family'] == 'PRODUCE') |\n",
    "          (csv['family'] == 'SEAFOOD')]\n",
    "\n",
    "# Clean Data, Keep 'store_nbr' related to Quito\n",
    "csv = csv[(csv['store_nbr'] == 1.0) |\n",
    "          (csv['store_nbr'] == 2.0) |\n",
    "          (csv['store_nbr'] == 3.0) |\n",
    "          (csv['store_nbr'] == 4.0) |\n",
    "          (csv['store_nbr'] == 6.0) |\n",
    "          (csv['store_nbr'] == 7.0) | \n",
    "          (csv['store_nbr'] == 8.0) |\n",
    "          (csv['store_nbr'] == 9.0) | \n",
    "          (csv['store_nbr'] == 10.0) |\n",
    "          (csv['store_nbr'] == 17.0) |\n",
    "          (csv['store_nbr'] == 18.0) |\n",
    "          (csv['store_nbr'] == 20.0) |\n",
    "          (csv['store_nbr'] == 44.0) |\n",
    "          (csv['store_nbr'] == 45.0) |\n",
    "          (csv['store_nbr'] == 46.0) |\n",
    "          (csv['store_nbr'] == 47.0) |\n",
    "          (csv['store_nbr'] == 48.0) |\n",
    "          (csv['store_nbr'] == 49.0)]\n",
    "\n",
    "csv = csv.drop_duplicates()\n",
    "print(csv.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e110b71e",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa766013",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Distribution: Average Sale\n",
    "\n",
    "# create a 3x3 grid of subplots\n",
    "f, axes = plt.subplots(3, 3, figsize=(18, 24))\n",
    "\n",
    "# list of 'family' names for the header\n",
    "family_names = ['BREAD/BAKERY', 'DAIRY', 'DELI', 'EGGS', 'FROZEN FOODS', \n",
    "                'MEATS', 'POULTRY', 'PREPARED FOODS', 'PRODUCE']\n",
    "\n",
    "# plot histograms for each 'family' in the grid\n",
    "for i, family in enumerate(family_names):\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    ax = sb.histplot(data=csv[csv['family'] == family], x='sales', ax=axes[row, col], kde=True)\n",
    "    ax.lines[0].set_color('crimson')\n",
    "    axes[row, col].set_title(family)\n",
    "    \n",
    "f, axes = plt.subplots(1, 1, figsize=(6, 6))\n",
    "    \n",
    "# plot histogram for 'SEAFOOD' outside the grid\n",
    "seafood_plot = sb.histplot(data=csv[csv['family'] == 'SEAFOOD'], x='sales', kde=True)\n",
    "seafood_plot.lines[0].set_color('crimson')\n",
    "seafood_plot.set_title('SEAFOOD')\n",
    "\n",
    "# display the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185c5198-33cc-41e7-89dc-1f1d9dd0a4dd",
   "metadata": {},
   "source": [
    "Based on the distribution curves, we can observe that the majority of the curves appear narrow, indicating that the data points are closely clustered together for each year between 2014 and 2017. This suggests that the sales within each category are relatively consistent over time, with little variation from year to year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b7b23c-7f66-4e1a-901d-1d039b5f7467",
   "metadata": {},
   "source": [
    "Next, we try to plot out the sales of each category over the years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6422ba67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LinePlot (Daily) of 'SEAFOOD'\n",
    "\n",
    "f, axes = plt.subplots(1, 1, figsize=(6, 6))\n",
    "seafood_plot = sb.lineplot(data=csv[csv['family'] == 'SEAFOOD'], x='date', y='sales')\n",
    "seafood_plot.set_title('SEAFOOD')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3367ad7-e24f-4b3d-a4e8-54b9e4185f7a",
   "metadata": {},
   "source": [
    "Due to the high number of dates, it was difficult to analyze the data from the plot. To address this, we resampled the data into weekly intervals instead of daily intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f12dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the index to a datetime index\n",
    "csv.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64a043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinePlot (Weekly)\n",
    "\n",
    "# create a 3x3 grid of subplots\n",
    "f, axes = plt.subplots(3, 3, figsize=(18, 24))\n",
    "\n",
    "# list of family names for the plots\n",
    "family_names = ['BREAD/BAKERY', 'DAIRY', 'DELI', 'EGGS', 'FROZEN FOODS', \n",
    "                'MEATS', 'POULTRY', 'PREPARED FOODS', 'PRODUCE']\n",
    "\n",
    "# iterate through the families and plot the weekly sales for each\n",
    "for i, family in enumerate(family_names):\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    # filter for the family\n",
    "    family_data = csv[csv['family'] == family]\n",
    "    # group by week and calculate mean sales\n",
    "    weekly_sales = family_data.resample('W').agg({'sales':'mean'})\n",
    "    # plot the line chart\n",
    "    sb.lineplot(data=weekly_sales, x=weekly_sales.index, y='sales', ax=axes[row, col])\n",
    "    # set the title for the subplot\n",
    "    axes[row, col].set_title(family)\n",
    "    # set the x-label for the subplot\n",
    "    axes[row, col].set_xlabel('Week')\n",
    "    # set the y-label for the subplot\n",
    "    axes[row, col].set_ylabel('Sales')\n",
    "\n",
    "# filter for the 'SEAFOOD' family\n",
    "family_data = csv[csv['family'] == 'SEAFOOD']\n",
    "\n",
    "# group the data by week and calculate the mean sales for each week\n",
    "weekly_sales = family_data.resample('W').agg({'sales':'mean'})\n",
    "\n",
    "# create a line plot of weekly sales for the 'SEAFOOD' family\n",
    "f, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "sb.lineplot(data=weekly_sales, x=weekly_sales.index, y='sales', ax=ax)\n",
    "ax.set_title('SEAFOOD')\n",
    "ax.set_xlabel('Week')\n",
    "ax.set_ylabel('Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb89fe1-ee7b-4a39-b097-6cb2e99055ef",
   "metadata": {},
   "source": [
    "From the time series plots, we can observe a general increase in food sales over the years. This observation supports our earlier findings from the distribution plots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db69088-6ef0-4c8f-9dad-148d11b43a41",
   "metadata": {},
   "source": [
    "## Correlation\n",
    "From the available data, there are other variables that we would like to investigate in order to determine if they have any correlation with food sales. If there is a correlation, we will attempt to incorporate those variables into our prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90322b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation of Onpromotion vs Sales (Weekly)\n",
    "\n",
    "# List of family names for the plots\n",
    "family_names = ['BREAD/BAKERY', 'DAIRY', 'DELI', 'EGGS', 'FROZEN FOODS', \n",
    "                'MEATS', 'POULTRY', 'PREPARED FOODS', 'PRODUCE']\n",
    "\n",
    "# Create a 3x3 grid of subplots\n",
    "f, axes = plt.subplots(3, 3, figsize=(18, 24))\n",
    "\n",
    "# Iterate through the families and plot the weekly sales for each\n",
    "for i, family in enumerate(family_names):\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    # Filter for the family\n",
    "    family_data = csv[csv['family'] == family]\n",
    "    # Group by week and calculate mean sales and onpromotion counts\n",
    "    weekly_sales = family_data.resample('W').agg({'sales':'mean', 'onpromotion':'sum'})\n",
    "    # Calculate the correlation coefficient\n",
    "    corr_coef = weekly_sales['sales'].corr(weekly_sales['onpromotion'])\n",
    "    # Create the scatter plot with best fit line and correlation coefficient\n",
    "    sb.regplot(data=weekly_sales, x='sales', y='onpromotion', ax=axes[row, col])\n",
    "    axes[row, col].set_title(f'{family}: Corr = {corr_coef:.2f}')\n",
    "    \n",
    "# Filter for the 'SEAFOOD' family\n",
    "family_data = csv[csv['family'] == 'SEAFOOD']\n",
    "# Group by week and calculate mean sales and onpromotion counts\n",
    "weekly_sales = family_data.resample('W').agg({'sales':'mean', 'onpromotion':'sum'})\n",
    "# Calculate the correlation coefficient\n",
    "corr_coef = weekly_sales['sales'].corr(weekly_sales['onpromotion'])\n",
    "# Create the scatter plot with best fit line and correlation coefficient in title\n",
    "f, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "sb.regplot(data=weekly_sales, x='sales', y='onpromotion', ax=ax)\n",
    "ax.set_title(f'SEAFOOD: Corr = {corr_coef:.2f}')\n",
    "ax.set_xlabel('Sales')\n",
    "ax.set_ylabel('Onpromotion')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98983958-098e-4a99-91ef-86044f28ed91",
   "metadata": {},
   "source": [
    "The plots and correlation calculations suggest that there is a moderate to high correlation for each category. Therefore, it appears that the variable \"onpromotion\" has an effect on food sales, and we should include it in our predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe754d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot of holiday vs Sales (Weekly) for Store 1\n",
    "\n",
    "# List of family names for the plots\n",
    "family_names = ['BREAD/BAKERY', 'DAIRY', 'DELI', 'EGGS', 'FROZEN FOODS', \n",
    "                'MEATS', 'POULTRY', 'PREPARED FOODS', 'PRODUCE']\n",
    "\n",
    "# Create a 3x3 grid of subplots\n",
    "f, axes = plt.subplots(3, 3, figsize=(18, 24))\n",
    "\n",
    "# Iterate through the families and plot the weekly sales for each\n",
    "for i, family in enumerate(family_names):\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    # Filter for the family\n",
    "    family_data = csv[csv['family'] == family]\n",
    "    family_data = family_data.loc[family_data['store_nbr'] == 1.0]\n",
    "    # Group by week and calculate mean sales and onpromotion counts\n",
    "    weekly_sales = family_data.resample('W').agg({'sales':'mean', 'holiday':'sum'})\n",
    "    # Calculate the correlation coefficient\n",
    "    corr_coef = weekly_sales['sales'].corr(weekly_sales['holiday'])\n",
    "    # Create the boxplot\n",
    "    sb.boxplot(data=weekly_sales, x='holiday', y='sales', ax=axes[row, col])\n",
    "    axes[row, col].set_title(f'{family}: Corr = {corr_coef:.2f}')\n",
    "    \n",
    "# Filter for the 'SEAFOOD' family\n",
    "family_data = csv[csv['family'] == 'SEAFOOD']\n",
    "family_data = family_data.loc[family_data['store_nbr'] == 1.0]\n",
    "# Group by week and calculate mean sales and onpromotion counts\n",
    "weekly_sales = family_data.resample('W').agg({'sales':'mean', 'holiday':'sum'})\n",
    "# Calculate the correlation coefficient\n",
    "corr_coef = weekly_sales['sales'].corr(weekly_sales['holiday'])\n",
    "# Create a box plot to visualize sales distribution\n",
    "f, ax = plt.subplots(figsize=(10, 6))\n",
    "sb.boxplot(data=weekly_sales, x='holiday', y='sales', ax=ax)\n",
    "ax.set_title(f'SEAFOOD: Correlation Coefficient = {corr_coef:.2f}')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970c54e3",
   "metadata": {},
   "source": [
    "The BoxPlot above suggests that higher holiday occurrences are associated with higher sales, despite the low correlation. Therefore, it is worthwhile to investigate whether this variable should be included in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21c3620-623f-45cc-9a44-78e65fabecf4",
   "metadata": {},
   "source": [
    "## Summary of EDA\n",
    "- In general, it appears that food sales have increased over the years, so we should expect current prices to be slightly higher than before.\n",
    "- Two variables that we should pay attention to are 'onpromotion', which represents the number of items on promotion for a given week, and 'holiday', which represents the number of holidays occurring in that week. These variables appear to be factors that influence food sales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e814e6ee-35ad-445d-8512-dccee0545a88",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "Our strategy for addressing this problem is to train the model to focus exclusively on a single food category. This approach should help the model generalize better than if we were to use data that includes multiple food categories. For the purposes of our example, we have selected the 'BREAD/BAKERY' category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a823de4c-9054-4e37-8db7-5c0f7e60bbfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select one \"Bread\" category and data from store 1 \n",
    "df = csv[csv[\"family\"] == \"BREAD/BAKERY\"]\n",
    "df = df[df[\"store_nbr\"] == 1]\n",
    "\n",
    "# Print df info\n",
    "df.info()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c488c8a6-88ef-46b4-bfeb-9c8013bb9da4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot time series sales data\n",
    "plt.figure(figsize=(30,30))\n",
    "plt.title(\"Sales of Bread products\", size=20)\n",
    "plt.plot(df[\"sales\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d70a90-3fb0-4e6a-9e55-30f06995a553",
   "metadata": {},
   "source": [
    "The plot illustrates the sales of bread/bakery products from January 2013 to September 2014, with time measured in days. Since the time series plot is difficult to interpret with so much data, we will be converting the time axis to weeks and aggregating the daily sales data into weekly means. This will provide a clearer picture of the sales trends over time and aid in data exploration and visualization. However, it's worth noting that the actual training data will still be in daily increments and only the visualization will be scaled down to weekly intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df516b0c-2958-4886-b084-7b46ace76bdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scale date range from days to week. Sales for the week is the mean\n",
    "resampled_df = df.resample('W').agg({'sales':'mean','onpromotion':'sum','holiday':'sum'})\n",
    "print(resampled_df)\n",
    "\n",
    "# Plot time series sales data\n",
    "plt.figure(figsize=(30,30))\n",
    "plt.title(\"Sales of Bread products\", size=20)\n",
    "plt.plot(resampled_df['sales'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6768b4f-4d36-4288-b45f-757e5b462f32",
   "metadata": {},
   "source": [
    "The plot allows us to more easily visualize the trends in the estimated sales data over time. From our observation, it is apparent that the sales price of the time series is non-stationary, which means that the mean and variance of the sales data change as time progresses. This characteristic makes it challenging to forecast future trends using traditional time-series forecasting methods, as time itself becomes a variable that affects the predictions. We will delve further into this issue in subsequent analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5438b040-488a-4c0c-97b2-3c0458aa2a67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data summary\n",
    "print(\"MEDIAN: \", df[\"sales\"].median())\n",
    "print(\"MEAN: \", df[\"sales\"].mean())\n",
    "print(\"SD: \", df[\"sales\"].std())\n",
    "print(\"MAX: \", df[\"sales\"].max())\n",
    "print(\"Min: \", df[\"sales\"].min())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612a2755-0a44-4578-aa7b-99039590b07a",
   "metadata": {},
   "source": [
    "## Intro to ARIMA model\n",
    "ARIMA is a popular time series model that combines three different models to analyze time series data: Auto Regressive (AR), Integrated (I), and Moving Average (MA). The name ARIMA reflects this combination of models.\n",
    "\n",
    "1) Auto Regressive(AR)\n",
    "It is a regression model that uses past values of the dependent variable (y) to predict future values.\n",
    "\n",
    "2) Moving Average(MA)\n",
    "This model analyzes the errors of the predicted values from the past and attempts to make more accurate predictions for the current time period.\n",
    "\n",
    "3) Integrated (I)\n",
    "Integrated (I) is not a separate model, but rather a method for dealing with non-stationary data. Stationary data satisfies three conditions: constant mean, constant standard deviation, and no seasonality. \n",
    "\n",
    "If the data is non-stationary, the I component is used to transform the data into a stationary state so that the AR and MA models can be applied.\n",
    "\n",
    "\n",
    "The goal of this notebook is to identify the optimal hyperparameters for the AR, I, and MA components of the ARIMA model. By identifying the best parameters, we can train a model that provides accurate forecasts for our time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ceaa55",
   "metadata": {},
   "source": [
    "# Stationarity of data (I-value)\n",
    "As previously mentioned, stationarity is a crucial aspect of time series modeling, as it ensures that the model can accurately predict future values. When a time series exhibits a trend, it becomes more challenging to predict future values accurately. Conversely, when the time series is stationary, it is easier to forecast future values since the data does not vary widely.\n",
    "\n",
    "From the plot above, we can observe that the sales data is not stationary. Specifically, we see that the sales of the store are increasing over time, which suggests the presence of a trend. However, we can confirm the stationarity of the data more accurately by using a statistical test such as the KPSS test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270e231b-ccee-4147-8c32-1137028e52f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import kpss\n",
    "#define KPSS\n",
    "def kpss_test(timeseries):\n",
    "    print ('Results of KPSS Test:')\n",
    "    kpsstest = kpss(timeseries, regression='c')\n",
    "    kpss_output = pd.Series(kpsstest[0:3], index=['Test Statistic','p-value','Lags Used'])\n",
    "    for key,value in kpsstest[3].items():\n",
    "        kpss_output['Critical Value (%s)'%key] = value\n",
    "    print (kpss_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6c9846-512e-439c-b7cb-4f686e45a182",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# KPSS test\n",
    "kpss_test(resampled_df[\"sales\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0501bf-81a5-442c-bf9c-3aa9e2c96457",
   "metadata": {},
   "source": [
    "The results of the KPSS test can be interpreted as follows:\n",
    "- If the test statistic is greater than the critical value, we reject the null hypothesis that the series is stationary. \n",
    "- If the test statistic is less than the critical value, we fail to reject the null hypothesis and conclude that the series is stationary.\n",
    "\n",
    "In the case of our data, the test statistic was found to be 1.989824, which exceeded the critical value at all confidence intervals. This result indicates that we must reject the null hypothesis and conclude that the series is not stationary. \n",
    "\n",
    "To make the sales data stationary, we will employ a technique called \"differencing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ab309c",
   "metadata": {},
   "source": [
    "# Differencing\n",
    "\n",
    "Differencing is a technique used to remove trends from non-stationary time series data. The process involves calculating the differences between consecutive data points by subtracting the value of the previous observation from the value of the next observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a4c938-ea16-4b06-8cf5-f1670cd449c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train test\n",
    "size = round(len(resampled_df)*0.8)\n",
    "\n",
    "train, test = resampled_df[0:size], resampled_df[size:len(resampled_df)]\n",
    "print(train,'\\n', test)\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea9d7df-2e04-423a-ae29-0bdeb497c266",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "difference_df = resampled_df.copy(deep=True)\n",
    "\n",
    "difference_df[\"sales\"] = difference_df[\"sales\"].diff()\n",
    "difference_df[\"sales\"] = difference_df[\"sales\"].diff()\n",
    "difference_df[\"sales\"] = difference_df[\"sales\"].diff()\n",
    "\n",
    "\n",
    "difference_df[\"sales\"].dropna().plot()\n",
    "difference_df = difference_df[[\"sales\", \"onpromotion\"]].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f4a89b",
   "metadata": {},
   "source": [
    "After performing differencing on the data, we can observe that the trend component of the time series has been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0d342f-028c-4c0d-874b-9a208cd22064",
   "metadata": {},
   "outputs": [],
   "source": [
    "kpss_test(difference_df[\"sales\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16273cc7",
   "metadata": {},
   "source": [
    "After performing differencing on the data, we can re-run the KPSS test to verify that the time series data is now stationary. If the KPSS test statistic is lower than the critical values at all confidence intervals, then we can conclude that the data is now stationary.\n",
    "\n",
    "The number of times we perform differencing is denoted as the 'I' value in the ARIMA model. In our case, we performed differencing thrice to achieve stationarity in the sales data. It's important to note that while stationarity can be achieved after a single round of differencing, our model showed improved performance with additional rounds of differencing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a08edf",
   "metadata": {},
   "source": [
    "# Finding MA value\n",
    "\n",
    "Now that we have determined the appropriate value for 'I' in our ARIMA model, we will proceed to identify the optimal value for the Moving Average (MA) component. We can use Auto-Correlation Function (ACF) analysis to accomplish this. ACF measures the correlation between a time series and a lagged version of itself. Take for example the image below:\n",
    "\n",
    "\n",
    "![](ACF.png)\n",
    "\n",
    "In a time series, the price in a given month (M) can be directly affected by the price in the preceding month (F), which in turn can be directly affected by the price in the month before that (J). This implies that January's price can indirectly impact March's price. However, January's price can also directly impact March's price. The Auto-Correlation Function (ACF) analysis helps us determine the best lagged period to use in our ARIMA model by measuring the correlation between each observation in the time series and its past lags. For instance, we can use the ACF plot to identify whether the price of January (t-2) or the price of February (t-1) better predicts the price of March (t).\n",
    "\n",
    "(Image taken from: https://towardsdatascience.com/interpreting-acf-and-pacf-plots-for-time-series-forecasting-af0d6db4061c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75be53bd-2598-41a4-b81b-12bc780b9de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACF test\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "plot_acf(difference_df[\"sales\"], lags = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0f4422",
   "metadata": {},
   "source": [
    "NOTE: When using ACF to identify the optimal value for the MA component, it's important to exclude lag 0 from the analysis since it represents the correlation between the time series and itself. This correlation is always equal to 1 and is therefore not useful in our analysis.\n",
    "\n",
    "From the ACF plot, we can observe that the highest correlation occurs at lag 1, indicating that the optimal value for the MA component in our ARIMA model is 1. We can ignore any correlation values that fall within the blue box, as these values are not statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e58ed5a",
   "metadata": {},
   "source": [
    "# Finding AR value\n",
    "To identify the optimal value for the Auto Regressive (AR) component in our ARIMA model, we can use Partial Auto-Correlation Function (PACF) analysis. PACF measures the correlation between a time series and its own lags, while removing the indirect effects of other lags. (In the example above, how price of Jan affect price of March.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71358d03-a5c6-47c8-bdf1-ff3222935a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PACF test\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "plot_pacf(difference_df[\"sales\"], lags = 30, method = \"ols\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dae34ec",
   "metadata": {},
   "source": [
    "As with ACF analysis, when using PACF to identify the optimal value for the AR component, we should exclude lag 0 from the analysis. From our PACF, we observe the highest correlation at lag 1, indicating that the optimal value for the AR component in our ARIMA model is 1. So that is our value for AR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4077d80d",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "\n",
    "After determining the optimal values for the AR, I, and MA components of our ARIMA model, we can train the model using the time series data. We can then use the trained model to make predictions about future sales trends.\n",
    "\n",
    "(Note that the ARIMA function given by statsmodel already does the differencing for us in the function, so we only need to feed in the raw data, not the differenced data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7647601d-a296-457b-9920-f5e466348877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "\n",
    "history = [x for x in train[\"sales\"]]\n",
    "predictions = []\n",
    "\n",
    "for t in range(len(test)):\n",
    "    \n",
    "    model = ARIMA(history, order=(1,3,1))\n",
    "    model_fit = model.fit()\n",
    "    output = model_fit.forecast()\n",
    "    yhat = output[0]\n",
    "    predictions.append(float(yhat))\n",
    "    obs = test[\"sales\"][t]\n",
    "\n",
    "    history.append(obs)\n",
    "    \n",
    "    print('predicted = %f, expected = %f' % (yhat, obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b81e35-6e4d-4c03-9504-e83f08e1f77f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions_series = pd.Series(predictions, index = test.index.shift(-1))\n",
    "fig,ax = plt.subplots(nrows = 1,ncols = 1,figsize = (15,5))\n",
    "\n",
    "plt.subplot(1,1,1)\n",
    "plt.plot(test[\"sales\"],label = 'Expected Values')\n",
    "plt.plot(predictions_series,label = 'Predicted values');\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc378196",
   "metadata": {},
   "source": [
    "This is the results of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80543483",
   "metadata": {},
   "source": [
    "# Evaluate using RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024a4cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "rmse = mean_squared_error(predictions_series, test['sales'], squared=False)\n",
    "print(\"RMSE: \", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ab05ae",
   "metadata": {},
   "source": [
    "# Adding of exog variables\n",
    "\n",
    "So far, we have used past values of the sales data to make predictions about future trends. However, we can also consider other variables that may have an impact on sales, such as the number of items on promotion and the occurrence of holidays. By incorporating these variables into our ARIMA model, we can create a more comprehensive and accurate forecasting tool. This involves adding these variables as exogenous inputs to the ARIMA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fcdc70-098d-45e6-86c8-adf2a5d82253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "\n",
    "exog_history = [x for x in train[\"sales\"]]\n",
    "exog_history_exog = [x for x in train[[\"onpromotion\", \"holiday\"]].to_numpy()]\n",
    "exog_predictions = []\n",
    "exog_future_exog = [x for x in test[['onpromotion', \"holiday\"]].to_numpy()]\n",
    "\n",
    "for t in range(len(test)):\n",
    "    \n",
    "    exog_model = ARIMA(exog_history, order=(1,3,1), exog=exog_history_exog)\n",
    "    exog_model_fit = exog_model.fit()\n",
    "    \n",
    "    exog_output = exog_model_fit.forecast(exog=exog_future_exog[t])\n",
    "    exog_yhat = exog_output[0]\n",
    "    exog_predictions.append(float(exog_yhat))\n",
    "    obs = test[\"sales\"][t]\n",
    "\n",
    "    exog_history.append(obs)\n",
    "    exog_history_exog.append(exog_future_exog[t])\n",
    "    \n",
    "    print('predicted = %f, expected = %f' % (exog_yhat, obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a486f9d8-b606-4a53-9fa5-459e55f81cf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exog_predictions_series = pd.Series(exog_predictions, index = test.index.shift(-1))\n",
    "fig,ax = plt.subplots(nrows = 1,ncols = 1,figsize = (15,5))\n",
    "\n",
    "plt.subplot(1,1,1)\n",
    "plt.plot(test[\"sales\"],label = 'Expected Values')\n",
    "plt.plot(exog_predictions_series,label = 'Predicted values');\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1664fbc-6ff5-44c4-9ea3-10d3a9cc2ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "exog_rmse = mean_squared_error(exog_predictions_series, test['sales'], squared=False)\n",
    "print(\"Exog_RMSE: \", exog_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b611818",
   "metadata": {},
   "source": [
    "# Comparison between with Exog variables and without"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15d2ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,1,1)\n",
    "plt.plot(test[\"sales\"],label = 'Expected Values')\n",
    "plt.plot(predictions_series,label = 'Predicted values');\n",
    "plt.plot(exog_predictions_series,label = 'Exog Predicted values');\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6109c01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = mean_squared_error(predictions_series, test['sales'], squared=False)\n",
    "exog_rmse = mean_squared_error(exog_predictions_series, test['sales'], squared=False)\n",
    "print(\"RMSE: \", rmse)\n",
    "print(\"Exog_RMSE: \",exog_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898e1188",
   "metadata": {},
   "source": [
    "After incorporating the exogenous variables into our ARIMA model, we observed a slight improvement in the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5399cd5-e7b0-4f40-8220-8b1508f75d47",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook, we managed to train a time series model (ARIMA) to predict food sales for specific categories on a weekly basis has the potential to help businesses better plan their inventory and reduce food waste by avoiding over-ordering. This can contribute to achieving Goal 12 of responsible consumption and production.\n",
    "\n",
    "Through exploratory data analysis (EDA), we were able to identify key trends and variables that impact food sales, such as the number of items on promotion and the occurrence of holidays. By incorporating these variables as exogenous inputs into our ARIMA model, we were able to improve its accuracy and reliability.\n",
    "\n",
    "Overall, our approach demonstrates the potential benefits of using data-driven insights and machine learning techniques to optimize business operations and improve sales forecasting. By doing so, businesses can make more informed decisions and ultimately contribute to sustainable and responsible consumption and production practices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519d1561",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
